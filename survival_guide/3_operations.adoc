= Fusion 5 Survival Guide: Day Two Operations
:toc:
:toclevels: 3
:toc-title:

// tag::body[]

This topic provides an overview of common daily maintenance tasks.

=== Helm upgrade script

// tag::upgrade-script[]

Once you
ifdef::env-github[]
link:2_planning.adoc[deploy a working cluster],
endif::[]
ifndef::env-github[]
link:/how-to/deploy-fusion-at-scale.html[deploy a working cluster],
endif::[]
we recommend using the upgrade script created by the `customize_fusion_values.sh` script. The upgrade script hard-codes the various parameters and alleviates the need to remember which parameters to pass to the script. This is especially helpful when working with multiple K8s clusters. Make sure you check the script into version control alongside your custom values YAML files.

Whenever you make a change to one of the custom values yaml files for your cluster, you need to run the upgrade script to apply the changes. The script simply calls `helm upgrade` with the correct parameters and `--values` options. Remember that if you run `helm upgrade` without passing the custom values yaml files, the deployment will revert back to using chart defaults, which you never want to do.

TIP: The script assumes your kubeconfig is pointing to the correct cluster; if not, it fails fast. Be sure to select the correct kubeconfig before running the script. The script also assumes the use of Helm v3.

// end::upgrade-script[]

=== Logging

// tag::logging[]

Fusion services come pre-configured to write log messages to stdout (common pattern in K8s) and send log messages to Logstash (deployed in our Helm chart).  Logstash is configured to index log messages into the Fusion `system_logs` collection.

The Log Viewer integrated into the Fusion Admin UI provides basic log analytics for the logs collected in `system_logs`.  You can also use your preferred log analytics stack, such as ELK, Splunk, DataDog, or similar.

For integration with an existing log infrastructure, you have two basic choices:

==== Disable Logstash and scrape logs from stdout

. In your custom values YAML files, set `logstashEnabled: false` for all services.
. Configure your logging infrastructure to scrape logs from the stdout from each pod.
+
This is a very common pattern in Kubernetes and most modern log analytics solutions have good integration with Kubernetes.
//In most cases, the customers ops team will help guide you on how they want this to work (typically with a log shipper process deployed as a DaemonSet on each node), there’s not much you’ll have to do.

==== Use Logstash to send logs

This is a good option if you use ELK but don't have an existing integration with Kubernetes.  This option requires some changes to the Logstash configuration in your custom values YAML file.
//Again, most would use something like Filebeat as a DaemonSet to do this vs. relying on our Logstash.

===== Elasticsearch

Elasticsearch is a common tool used to store and query logs and can be easily configured as an output for Logstash. Just include the `elasticsearchHost` to our Helm chart and set it to `host:port`. In most cases, the port number will be 9200, as in the example below.

[source,yaml]
----
logstash:
   elasticsearchHost: <elasticsearch-host>:9200
----

// end::logging[]

=== Grafana dashboards

// tag::grafana[]

If you're running Prometheus and Grafana for monitoring the performance and health of Fusion services, then you'll want to install Lucidworks Grafana dashboards. We provide a number of dashboards in the link:https://github.com/lucidworks/fusion-cloud-native/tree/master/monitoring/grafana[fusion-cloud-native repo^].

.How to configure Grafana

. If you used the `install_prom.sh` script to install Prometheus / Grafana into your Fusion namespace, then you need to get the initial Grafana password from a K8s secret by doing:
+
[source,bash]
----
kubectl get secret --namespace "${NAMESPACE}" ${RELEASE}-monitoring-grafana \
  -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
----
To see the name of the Grafana deployment, do: `kubectl get deploy`

. With Grafana, you can either set up a temporary port-forward to a Grafana pod or expose Grafana on an external IP using a K8s `LoadBalancer`. To define a `LoadBalancer`, do (replace `${RELEASE}` with your Helm release label, typically the same as your namespace):
+
[source,bash]
----
kubectl expose deployment ${RELEASE}-monitoring-grafana --type=LoadBalancer --name=grafana --port=3000 --target-port=3000
----

. Use `kubectl get services --namespace <namespace>` to verify that the load balancer is set up and has an external IP address.

. Direct your browser to `\http://<GrafanaIP>:3000` and enter the username `admin@localhost` and the password that was returned in the previous step.
+
This will log you into the application. It is recommended that you create another administrative user with a more desirable password.
+
If you used the `install_prom.sh` script, then a default Prometheus data source will already be configured for you.
If not, then you need to configure the Prometheus data source in Grafana.

. Go to the gear icon on the left and then go to *Data Sources*.

. Click *Add Data Source* and then click on *Prometheus* as the data source type. It will bring you to a page where it will ask for HTTP URL for the Prometheus server.
+
image:https://github.com/lucidworks/fusion-cloud-native/blob/master/survival_guide/grafana-add-datasource.png?raw=true[]
. Enter `\http://<RELEASE>-prom-prometheus-server`

. Configure any additional fields as desired (but defaults are fine), then click *Save and Test*, which should succeed.

. If you used the `install_prom.sh` script, then Fusion's default Grafana dashboards will already be imported. If not, import the dashboards from the fusion-cloud-native repo:
+
image:https://github.com/lucidworks/fusion-cloud-native/blob/master/survival_guide/grafana-import.png?raw=true[]
// end::grafana[]

=== Resource limits

// tag::resources[]

Lucidworks recommends installing Fusion without resource limits initially as they can over-complicate the initial setup of your cluster, especially for proof-of-concept / getting started clusters. Resource requests / limits directly impact the number of nodes needed to deploy Fusion. Once your installation is up and running with a critical mass of data, then you can start to fine-tune resource limits for Fusion services.

For production like environments, you should define resource limits to help K8s schedule pods correctly across the nodes in your cluster. This is especially important for K8s clusters that host other namespaces besides Fusion.

If you used the `--with-resource-limits` option when running the `./customize_fusion_values.sh` script, then you already have resource limits configured for your cluster.

Look for a file named `<provider>_<cluster>_<namespace>_fusion_resources.yaml`; if you do not have this file, simply copy https://github.com/lucidworks/fusion-cloud-native/blob/master/example-values/resources.yaml[`resources.yaml`] to help you get started with setting the appropriate resource limits.

You can refine the resource requests / limits as you test your cluster's behavior while preparing to go to production with Fusion.

// end::resources[]

=== Pod affinity rules

// tag::affinity[]

Affinity rules govern how pods for Fusion components are scheduled across the cluster. All components have the same affinity setup which follows this logic:

* When scheduling, prefer to put a pod on a node that is in an availability zone that doesn't already have a running instance of this component.

* Require that pods are all deployed on a host that doesn't have a running instance of the component that is being scheduled.

This means that the loss of a host will bring down at most one component. However, the cluster will need to be at least as large as the number of replicas in the largest deployment.

If you need to run a large number of a certain type of component, then consider relaxing the "required" policy by changing it to a "preferred" policy on hostname by changing

----
     requiredDuringSchedulingIgnoredDuringExecution:
----
to
----
     preferredDuringSchedulingIgnoredDuringExecution:
----

for the `kubernetes.io/hostname` policies.

If you used the `--with-affinity-rules` option when running the `./customize_fusion_values.sh` script, then you already have pod affinity rules configured for your cluster. If not, then we recommend copying the https://github.com/lucidworks/fusion-cloud-native/blob/master/example-values/affinity.yaml[`example-values/affinity.yaml` file^] and renaming it using our convention: `<provider>_<cluster>_<release>_fusion_affinity.yaml`.

Append the following to your
ifdef::env-github[]
link:#helm-upgrade-script[upgrade script]:
endif::[]
ifndef::env-github[]
link:/how-to/create-helm-upgrade-script.html[upgrade script]:
endif::[]

----
MY_VALUES="${MY_VALUES} --values gke_search_f5_fusion_affinity.yaml"
----

// end::affinity[]

=== Multiple replicas and horizontal pod auto-scaling

// tag::auto-scaling[]

You can configure multiple replicas and horizontal pod autoscalers (tied to CPU usage) for Fusion components.

If you used the `--with-replicas` option when running the `./customize_fusion_values.sh` script, then you already have replicas configured for your cluster.

If not, then copy the example file (`example-values/replicas.yaml`) and rename it using our convention: `<provider>_<cluster>_<release>_fusion_replicas.yaml`

Append the following to your
ifdef::env-github[]
link:#helm-upgrade-script[upgrade script]:
endif::[]
ifndef::env-github[]
link:/how-to/create-helm-upgrade-script.html[upgrade script]:
endif::[]

----
MY_VALUES="${MY_VALUES} --values gke_search_f5_fusion_replicas.yaml"
----

=== Tune Fusion Application Performance

In this section, we cover a variety of topics to help you get the best Search performance for your Fusion application.

If you have not created an application yet, proceed to the Fusion Admin UI to create your first application. For the purposes of this section, we'll use a sample application named `dcommerce`.

==== Fix Solr Collection Replica Placement

If you're using multiple Solr StatefulSets, such as to partition Solr pods into `search`, `analytics`, and `system` pools, then you need to use a Solr auto-scaling policy to govern replica placement for Fusion collections.

Open a port-forward to a Solr pod in the cluster.
----
kubectl port-forward <SOLR_POD_ID> 8983
----

Inspect the Solr auto-scaling policy in the link:https://github.com/lucidworks/fusion-cloud-native/blob/master/policy.json[policy.json^] file. The syntax is rather cryptic, but it basically defines a separate policy for search, analytics, and system oriented collections.

Run the `./update_policy.sh` script to add the Solr auto-scaling policy from policy.json into the Solr cluster.

Unfortunately, due to a limitation in Solr (https://issues.apache.org/jira/browse/SOLR-14347), replicas do not get placed correctly for Solr collections created by Fusion during application creation.

Consequently, you'll need to delete the Solr collections and re-create them using a BASH script.

The recommended approach is to adapt the link:https://github.com/lucidworks/fusion-cloud-native/blob/master/update_app_coll_layout.sh[update_app_coll_layout.sh^] script for your application, such as setting the correct number of shards, replicas, replica types, and policy for each collection used by your Fusion application.
Make a copy of the `update_app_coll_layout.sh` script and set the vars at the top for the specific app, in this case `dcommerce`.

For this example, we'll use the following settings:

[width="90%",cols="4,2,5,2",options="header"]
|=========================================================
|Collection|Shards|Replicas|Policy
|dcommerce|1|2 tlog, 3 pull|search
|dcommerce_signals_aggr|1|2 tlog, 3 pull|search
|dcommerce_query_rewrite|1|2 tlog, 3 pull|search
|dcommerce_user_prefs|1|2 nrt|search
|dcommerce_signals|3|2 nrt|analytics
|dcommerce_query_rewrite_staging|1|2 nrt|analytics
|dcommerce_job_reports|1|2 nrt|analytics
|=========================================================

Here's an example for our `dcommerce` app, adjust to meet your specific use case:
----
#!/bin/bash

APP="dcommerce"
SOLR="http://localhost:8983"

curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_signals"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_signals_aggr"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_query_rewrite_staging"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_query_rewrite"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_job_reports"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_user_prefs"

# analytics oriented collections
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_signals&collection.configName=${APP}_signals&numShards=3&replicationFactor=2&policy=analytics&maxShardsPerNode=2"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_query_rewrite_staging&collection.configName=${APP}_query_rewrite_staging&numShards=1&replicationFactor=2&policy=analytics"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_job_reports&collection.configName=${APP}_job_reports&numShards=1&replicationFactor=2&policy=analytics"

# search oriented collections
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}&collection.configName=${APP}&numShards=1&tlogReplicas=2&pullReplicas=3&policy=search"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_signals_aggr&collection.configName=${APP}_signals_aggr&numShards=1&tlogReplicas=2&pullReplicas=3&policy=search"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_query_rewrite&collection.configName=${APP}_query_rewrite&numShards=1&tlogReplicas=2&pullReplicas=3&policy=search"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_user_prefs&collection.configName=${APP}_user_prefs&numShards=1&replicationFactor=2&policy=search"
----

Notice that script deletes Solr collections and re-creates them with the correct auto-scaling policy in place. Obviously, you should not run this on collections that have data without backing up the data first.

For more information about Solr replica types, see: https://lucene.apache.org/solr/guide/8_4/shards-and-indexing-data-in-solrcloud.html#types-of-replicas

==== Tune Solr Commit Settings

Fusion collections are created with a default commit within set to 10 secs. This overrides the commit settings set for a collection in the `solrconfig.xml`.

Commit within 10 seconds is too aggressive for production environments as it will cause Solr to open a new search and flush all caches.
For environments where optimal performance is important, you may want to disable the commit within setting for your collections and instead rely solely on auto soft and hard commits.

Disable commit within using the `update_commit_within_f5.sh` script, for instance:
----
./update_commit_within_f5.sh --collection dcommerce --gateway GATEWAY_URL --commit_within -1
----
Replace `GATEWAY_URL` with the URL of the K8s Ingress or IP for the Fusion API Gateway. Repeat this process for all Fusion collections.

TIP: You can get the IP of the Gateway pod using: `export LW_K8S_GATEWAY_IP=$(kubectl --namespace ${LW_K8S_NAMESPACE} get service proxy -o jsonpath='{.status.loadBalancer.ingress[0].ip}')`

Configure soft / hard auto commit settings in solrconfig.xml (via the Fusion Admin UI), such as:
----
    <autoCommit>
      <maxTime>60000</maxTime>
      <openSearcher>false</openSearcher>
    </autoCommit>

    <autoSoftCommit>
      <maxTime>300000</maxTime>
    </autoSoftCommit>
----
You want the auto soft-commit setting to be as long as possible (in millis) to avoid re-opening searchers too often, which invalidates your caches.

You should also consider disabling commits / optimize requests coming from external client applications by configuring the `IgnoreCommitOptimizeUpdateProcessorFactory` in your update processor chain(s).

----
    <processor class="solr.IgnoreCommitOptimizeUpdateProcessorFactory">
      <int name="statusCode">200</int>
      <str name="responseMessage">Thou shall not issue a commit!</str>
    </processor>
----
This prevents external client applications that you do not control from committing (or optimizing) too often. For most production environments, you should rely solely on the auto-commit settings in solrconfig.xml.

==== Enable Buffering for Index Pipelines

For each index pipeline, ensure the `Buffer Documents and Send Them to Solr in Batches` option is enabled for the Solr Index stage.

==== Tune Solr Cache Settings

Solr has a number of caches, such as the filter cache, that have a major impact on performance. For many production environments, the max size for these caches is too small and should be increased.
Be sure to look at the metrics for your caches after running load tests to determine if you need to tune them. Cache configuration is done in the solrconfig.xml for each collection using the Fusion Admin UI.

Typically the three most important caches to tune are:
----
    <filterCache class="solr.FastLRUCache"
                 size="5000"
                 maxRamMB="64"
                 autowarmCount="0"/>

    <queryResultCache class="solr.LRUCache"
                      size="6000"
                      maxRamMB="250"
                      autowarmCount="0"/>

    <documentCache class="solr.LRUCache"
                   size="25000"
                   maxRamMB="64"
                   autowarmCount="0"/>
----

TIP: Be careful with `autowarmCount` as that will impact how long it takes for a new searcher to open.

==== Query Pipeline Routing Parameters

If you're using a separate `search` pool for search oriented collections, then you'll want to add the `lw.nodeFilter=host:solr-search` parameter to the main query pipeline(s) to ensure queries get routed from Fusion to Solr Search pods only.

If you're using PULL replicas for search collections, then you should also pass `shards.preference=replica.type:PULL,replica.location:local` to Solr.

This ensures that queries get routed to PULL replicas only and favors the local replica if it exists. For more information about `shards.preference`, see:
https://lucene.apache.org/solr/guide/8_4/distributed-requests.html#shards-preference-parameter

You should also provide these parameters for sidecar queries, such as in the tagger, rules, and signals boost stages.

==== Async Query Stages

The tagger and rules stages can be configured with a max time constraint that enforces an upper bound on how long these stages can take. Behind the scenes, this requires executing the sidecar request in a background thread.

In addition, it's common to configure your pipeline to do the rules lookup and signals boost concurrently using Fusion asynchronous stage support. If you're using these features, please ensure you pass the following Java system property:
----
-Djava.util.concurrent.ForkJoinPool.common.parallelism=1
----

=== Use Gatling to run query performance / load tests

Lucidworks recommends running query performance tests to establish a baseline number of pods for the proxy, query pipeline, and Solr services. You can use the gatling-qps project provided in the link:https://github.com/lucidworks/fusion-cloud-native[fusion-cloud-native repo^] as a starting point for building a query load test. Gatling.io is a load test framework that provides a powerful Scala-based DSL for constructing performance test scenarios. See `FusionQueryTraffic.scala` in the repo as a starting point for building query performance tests for Fusion 5.

==== Register warming queries

To avoid any potential delays when a new query pod joins the cluster, such as in reaction to an HPA auto-scaling trigger, we recommend registering a small set of queries to "warm up" the query pipeline service before it gets added to the Kubernetes service. In the query-pipeline section of the custom values YAML, configure your warming queries using the structure shown in the example below:

[source,json]
----
warmingQueryJson:
  {
  "pipelines": [
    {
      "pipeline": "<PIPELINE>",
      "collection": "<COLLECTION>",
      "params": {
        "q": ["*:*"]
      }
    },{
      "method" : "POST",
      "pipeline": "<ANOTHER_PIPELINE>",
      "collection": "<ANOTHER_COLL>",
      "params": {
        "q": ["*:*"]
      }
    }
  ],
  "profiles": [
    {
      "profile": "<PROFILE>",
      "params": {
        "q": ["*:*"]
      }
    }
  ]
  }
----

NOTE: The indentation for the opening / closing braces is important for embedding JSON in YAML

// end::auto-scaling[]


[[spark-ops]]
=== Spark operations

In Fusion 5.x, Spark operates in native Kubernetes mode instead of standalone mode (like in Fusion 4.x). The sections below describe Spark operations in Fusion 5.0.

// tag::spark-ops-intro[]

==== Node Selectors

You can control which nodes Spark executors are scheduled on using Spark configuration property for a job:
----
spark.kubernetes.node.selector.<LABEL>=<LABEL_VALUE>
----
For instance, if a node is labeled with `fusion_node_type=spark_only`, then you would scheduled Spark executor pods to run on that node using:
----
spark.kubernetes.node.selector.fusion_node_type=spark_only
----

TIP: Spark version 2.4.x does not support tolerations for Spark pods; consequently, Spark pods cannot be scheduled on any nodes with taints.

==== Cluster mode

Fusion 5.0 ships with Spark 2.4.3 and operates in "cluster" mode on top of Kubernetes. In cluster mode, each Spark driver runs in a separate pod and hence resources can be managed per job. Each executor also runs in its own pod.

==== Spark config defaults

The table below shows the default configurations for Spark. These settings are configured in the job-launcher config map, accessible using `kubectl get configmaps <release-name>-job-launcher`. Some of these settings are also configurable via Helm.

.Spark Resource Configurations
[cols="2m,1a,1m"]
|===
|Spark Configuration
|Default value
|Helm Variable

|spark.driver.memory
|3g
|

|spark.executor.instances
|2
|executorInstances

|spark.executor.memory
|3g
|

|spark.executor.cores
|6
|

|spark.kubernetes.executor.request.cores
|3
|

|===


.Spark Kubernetes Configurations
[cols="2m,1a,1m"]
|===
|Spark Configuration
|Default value
|Helm Variable

|spark.kubernetes.container.image.pullPolicy
|Always
|image.imagePullPolicy

|spark.kubernetes.container.image.pullSecrets
|[artifactory]
|image.imagePullSecrets

|spark.kubernetes.authenticate.driver.serviceAccountName
|<name>-job-launcher-spark
|

|spark.kubernetes.driver.container.image
|fusion-dev-docker.ci-artifactory.lucidworks.com
|image.repository

|spark.kubernetes.executor.container.image
|fusion-dev-docker.ci-artifactory.lucidworks.com
|image.repository

|===

// end::spark-ops-intro[]

==== Spark job resource allocation

//tag::spark-resources[]

===== Number of instances and cores allocated

In order to set the number of cores allocated for a job, add the following parameter keys and values in the Spark Settings field within the "advanced" job properties in the Fusion UI or the `sparkConfig` object if defining a job via the Fusion API.

If `spark.kubernetes.executor.request.cores` is not set (default config), then Spark will set the number of CPUs for the executor pod to be the same number as `spark.executor.cores`. In that case, if `spark.executor.cores` is 3, then Spark will allocate 3 CPUs for the executor pod and will run 3 tasks in parallel. To under-allocate the CPU for the executor pod and still run multiple tasks in parallel, set `spark.kubernetes.executor.request.cores` to a lower value than `spark.executor.cores`.

The ratio for `spark.kubernetes.executor.request.cores` to `spark.executor.cores` depends on the type of job: either CPU-bound or I/O-bound. Allocate more memory to the executor if more tasks are running in parallel on a single executor pod.

[cols="3m,1a"]
|===
|Parameter Key
|Example Value

|spark.executor.instances
|3

|spark.kubernetes.executor.request.cores
|3

|spark.executor.cores
|6

|spark.driver.cores
|1

|===

If these settings are left unspecified, then the job launches with a driver using one core and 3GB of memory plus two executors, each using one core with 1GB of memory.

===== Memory allocation

The amount of memory allocated to the driver and executors is controlled on a per-job basis using the `spark.executor.memory` and `spark.driver.memory` parameters in the Spark Settings section of the job definition in the Fusion UI or within the `sparkConfig` object in the JSON definition of the job.

[cols="3m,1a"]
|===
|Parameter Key
|Example Value

|spark.executor.memory
|6g

|spark.driver.memory
|2g

|===

//end::spark-resources[]

==== Configuring credentials in the Kubernetes cluster

//tag::spark-credentials[]

AWS/GCS credentials can be configured per job or per cluster.

===== Configuring GCS credentials for Spark jobs

. Create a secret containing the credentials JSON file.
+
See https://cloud.google.com/iam/docs/creating-managing-service-account-keys on how to create service account JSON files.
+
[source,bash]
----
kubectl create secret generic solr-dev-gc-serviceaccount-key --from-file=/Users/kiranchitturi/creds/solr-dev-gc-serviceaccount-key.json
----

. Create an extra config map in Kubernetes setting the required properties for GCP.
.. Create a properties file with GCP properties:
+
[source,bash]
----
$ cat gcp-launcher.properties
spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
spark.kubernetes.driver.secrets.solr-dev-gc-serviceaccount-key = /mnt/gcp-secrets
spark.kubernetes.executor.secrets.solr-dev-gc-serviceaccount-key = /mnt/gcp-secrets
spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
spark.hadoop.google.cloud.auth.service.account.json.keyfile = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
----
.. Create a config map based on the properties file:
+
[source,bash]
----
kubectl create configmap gcp-launcher --from-file=gcp-launcher.properties
----
. Add the gcp-launcher config map to values.yaml under job-launcher:
+
[source,yaml]
----
configSources: gcp-launcher
----

===== Configuring S3 credentials for Spark jobs

AWS credentials can’t be set via a single file. So, we have to set two environment variables referring to the key and secret.

. Create a secret pointing to the creds:
+
[source,bash]
----
kubectl create secret generic aws-secret --from-literal=key='<access key>' --from-literal=secret='<secret key>'
----
. Create an extra config map in Kubernetes setting the required properties for AWS:
.. Create a properties file with AWS properties:
+
[source,bash]
----
cat aws-launcher.properties
spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key
spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret
spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key
spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret
----
.. Create a config map based on the properties file:
+
[source,bash]
----
kubectl create configmap aws-launcher --from-file=aws-launcher.properties
----
. Add the `aws-launcher` config map to `values.yaml` under `job-launcher`:
+
[source,yaml]
----
configSources: aws-launcher
----

===== Configuring Azure Data Lake credentials for Spark jobs

Configuring Azure through environment variables or `configMaps` does not seem to be possible at the moment. You need to manually upload the `core-site.xml` file into the job-launcher pod at `/app/spark-dist/conf`.

Currently only Data Lake Gen 1 is supported.

Here’s what the `core-site.xml` file should look like:
[source,xml]
----
<property>
  <name>dfs.adls.oauth2.access.token.provider.type</name>
  <value>ClientCredential</value>
</property>
<property>
    <name>dfs.adls.oauth2.refresh.url</name>
    <value> Insert Your OAuth 2.0 Endpoint URL Value Here </value>
</property>
<property>
    <name>dfs.adls.oauth2.client.id</name>
    <value> Insert Your Application ID Here </value>
</property>
<property>
    <name>dfs.adls.oauth2.credential</name>
    <value>Insert the Secret Key Value Here </value>
</property>
<property>
    <name>fs.adl.impl</name>
    <value>org.apache.hadoop.fs.adl.AdlFileSystem</value>
</property>
<property>
    <name>fs.AbstractFileSystem.adl.impl</name>
    <value>org.apache.hadoop.fs.adl.Adl</value>
</property>
----

===== Configuring credentials per job

. Create a Kubernetes secret with the GCP/AWS credentials.
. Add the Spark configuration to configure the secrets for the Spark driver/executor.

====== GCS

. Create a secret containing the credentials JSON file.
+
See https://cloud.google.com/iam/docs/creating-managing-service-account-keys on how to create service account JSON files.
+
[source,bash]
----
kubectl create secret generic solr-dev-gc-serviceaccount-key --from-file=/Users/kiranchitturi/creds/solr-dev-gc-serviceaccount-key.json
----
. Toggle the Advanced config in the job UI and add the following to the Spark configuration:
+
----
spark.kubernetes.driver.secrets.solr-dev-gc-serviceaccount-key = /mnt/gcp-secrets
spark.kubernetes.executor.secrets.solr-dev-gc-serviceaccount-key = /mnt/gcp-secrets
spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
spark.hadoop.google.cloud.auth.service.account.json.keyfile = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
----

====== S3

AWS credentials can’t be set via a single file. So, we have to set two environment variables referring to the key and secret.

. Create a secret pointing to the creds:
+
----
kubectl create secret generic aws-secret --from-literal=key='<access key>' --from-literal=secret='<secret key>'
----
. Toggle the Advanced config in the job UI and add the following to Spark configuration:
+
----
spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key
spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret
spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key
spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret
----

//end::spark-credentials[]

==== How to get logs for a Spark job

// tag::spark-logs[]

* To get the initial logs that contain information about the pod spin up, do:
+
----
curl -X GET -u admin:password123 http://localhost:8764/api/apollo/spark/driver/log/{jobId}
----
* Get the pod ID by running:
+
----
k get pods -l spark-role=driver -l jobConfigId=<job-id>
----
* Logs from failed jobs can be obtained by using:
+
----
kubectl logs [DRIVER-POD-NAME]
----
* Logs from running containers can be tailed using the `-f` parameter:
+
----
kubectl logs -f [POD-NAME]
----

Spark deletes failed and successful executor pods. Fusion provides a cleanup Kubernetes cron job that removes successfully completed driver pods every 15 minutes.

// end::spark-logs[]

==== Pod cleanup

//tag::pod-cleanup[]

Spark driver pods are cleaned up using a Kubernetes cron job that runs every 15 minutes to clean up pods using this command:
----
kubectl delete pods --namespace default --field-selector=status.phase=Succeeded -l spark-role=driver
----
This cron job is created automatically when the `job-launcher` microservice is installed in the Fusion cluster.

//end::pod-cleanup[]

==== Spark History Server

//tag::history-intro[]
While logs from the Spark driver and executor pods can be viewed using `kubectl logs [POD_NAME]`, executor pods are deleted at their end of their execution, and driver pods are deleted by Fusion on a default schedule of every hour. In order to store and view Spark logs in a more long-term fashion, you can install the Spark History Server into your Kubernetes cluster and configure Spark to write logs in a manner that will persist.
//end::history-intro[]

===== Installing Spark History Server

//tag::history-install[]

Spark History Server can be installed via its default Helm chart:
----
helm install stable/spark-history-server --values values.yaml
----

//end::history-install[]

//tag::history-config[]

===== Recommended Configuration

Our recommended configuration for using the Spark History Server with Fusion is to store and read Spark logs in cloud storage. For installations on Google Kubernetes Engine, we suggest setting these keys in the `values.yaml`:
[source,yaml]
----
gcs:
    enableGCS: false
    secret: history-secrets
    key: [SECRET_KEY_NAME].json
    logDirectory: gs://[BUCKET_NAME]
service:
    type: ClusterIP
    port: 18080
----
We override the default `service.type` of `LoadBalancer` with `ClusterIP` to keep the History Server from being accessible to outside connections without port-forwarding.

You may need to set up your secret for full access to the cloud bucket:
[source,bash]
----
$ export ACCOUNT_NAME=[SECRET_KEY_NAME]
$ export GCP_PROJECT_ID=[PROJECT_ID]
$ gcloud iam service-accounts create ${ACCOUNT_NAME} --display-name "${ACCOUNT_NAME}"
$ gcloud iam service-accounts keys create "${ACCOUNT_NAME}.json" --iam-account "${ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com"
$ gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} --member "serviceAccount:${ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com" --role roles/storage.admin
$ gsutil iam ch serviceAccount:${ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com:objectAdmin gs://[BUCKET_NAME]
----
The service key sets up the history server on an internal IP within your cluster but does not create a LoadBalancer (which is the default setting in the Spark History Server Helm chart). This prevents the server from being exposed to outside access by default. We’ll look at how to access the history server shortly.

===== Other Configurations

====== Azure

[source,bash]
----
$ echo "your-storage-account-name" >> azure-storage-account-name
$ echo "your-container-name" >> azure-blob-container-name
# to auth with sas token (if wasbs.sasKeyMode=true, which is the default)
$ echo "your-azure-blob-sas-key" >> azure-blob-sas-key
# or to auth with storage account key
$ echo "your-azure-storage-account-key" >> azure-storage-account-key
$ kubectl create secret generic azure-secrets --from-file=azure-storage-account-name --from-file=azure-blob-container-name [--from-file=azure-blob-sas-key | --from-file=azure-storage-account-key]
----

For SAS token access, `values.yaml` should look like:
[source,yaml]
----
wasbs:
    enableWASBS: true
    secret: azure-secrets
    sasKeyName: azure-blob-sas-key
    storageAccountNameKeyName: azure-storage-account-name
    containerKeyName: azure-blob-container-name
    logDirectory: [BUCKET-NAME]
----
For non-SAS access:
[source,yaml]
----
wasbs:
    enableWASBS: true
    secret: azure-secrets
    sasKeyMode: false
    storageAccountKeyName: azure-storage-account-key
    storageAccountNameKeyName: azure-storage-account-name
    containerKeyName:  azure-blob-container-name
    logDirectory: [BUCKET-NAME]
----

====== AWS

The recommended approach for S3 access is to use AWS IAM roles, but you can also use a access/secret key pair as a Kubernetes secret:

[source,bash]
----
$ aws iam list-access-keys --user-name your-user-name --output text | awk '{print $2}' >> aws-access-key
$ echo "your-aws-secret-key" >> aws-secret-key
$ kubectl create secret generic aws-secrets --from-file=aws-access-key --from-file=aws-secret-key
----

For IAM, your `values.yaml` will be:

[source,yaml]
----
s3:
    enableS3: true
    logDirectory: s3a://[BUCKET-NAME]
----
(Note the Hadoop `s3a://` link instead of `s3://`.)

With a access/secret pair, you’ll need to add the secret:

[source,yaml]
----
s3:
    enableS3: true
    enableIAM: false
    accessKeyName: aws-access-key
    secretKeyName: aws-secret-key
    logDirectory: s3a://[BUCKET-NAME]
----

===== Configuring Spark

After the History Server has been set up, then the Fusion job-launcher deployment ConfigMap’s `application.yaml` key will need to be updated with these Spark settings so the driver and executors know where to write out their logs. In this example, we’re redirecting to a GCS bucket:

[source,yaml]
----
spark:
    eventLog:
        enabled: true
        compress: true
        dir: gs://[BUCKET-NAME]
    hadoop:
        google:
            cloud:
                auth:
                    service:
                        account:
                            json:
                                keyfile: /etc/secrets/[SECRET_KEY_NAME].json]
    kubernetes:
        driver:
            secrets:
                history-secrets: /etc/secrets
        executor:
            secrets:
                history-secrets: /etc/secrets
----

//end::history-config[]

===== Accessing The Spark History Server

//tag::history-access[]

As we have set up the History Server to only set up a ClusterIP, we will need to port forward the server using `kubectl`:
----
kubectl get pods # to find the Spark History Server pod
kubectl port-forward [POD_NAME] 18080:18080
----

You can now access the Spark History Server at `\http://localhost:18080`.

//end::history-access[]

=== Upgrades with Helm v3

// tag::upgrades[]

One of the most powerful features provided by Kubernetes and a cloud-native microservices architecture is the ability to do a rolling update on a live cluster. Fusion 5 allows customers to upgrade from Fusion 5.0.2 to a later 5.x.y version on a live cluster with zero downtime or disruption of service.

When Kubernetes performs a rolling update to an individual microservice, there will be a mix of old and new services in the cluster concurrently (only briefly in most cases) and requests from other services will be routed to both versions. Consequently, Lucidworks ensures all changes we make to our service do not break the API interface exposed to other services in the same 5.x line of releases. We also ensure that the stored configuration remains compatible in the same 5.x release line.

Lucidworks releases minor updates to individual services frequently, so you can can pull in those upgrades using Helm at your discretion.

.How to upgrade Fusion
. Clone the https://github.com/lucidworks/fusion-cloud-native[fusion-cloud-native repo^], if you haven't already.
. Locate the `setup_f5_<platform>.sh` script that matches your Kubernetes platform.
. Run the script with the `--upgrade` option.
+
TIP: To see what would be upgraded, pass the `--dry-run` option to the script.

The scripts in the fusion-cloud-native repo automatically pull in the latest chart updates from our Helm repository and deploy any updates needed by doing a diff of your current installation and the latest release from Lucidworks.

// end::upgrades[]

// end::body[]
